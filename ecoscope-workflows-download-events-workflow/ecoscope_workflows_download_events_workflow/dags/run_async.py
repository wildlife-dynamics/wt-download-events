# AUTOGENERATED BY ECOSCOPE-WORKFLOWS; see fingerprint in README.md for details
import json
import os

from ecoscope_workflows_core.graph import DependsOn, Graph, Node
from ecoscope_workflows_core.tasks.config import set_string_var as set_string_var
from ecoscope_workflows_core.tasks.config import (
    set_workflow_details as set_workflow_details,
)
from ecoscope_workflows_core.tasks.filter import (
    get_timezone_from_time_range as get_timezone_from_time_range,
)
from ecoscope_workflows_core.tasks.filter import set_time_range as set_time_range
from ecoscope_workflows_core.tasks.groupby import set_groupers as set_groupers
from ecoscope_workflows_core.tasks.groupby import split_groups as split_groups
from ecoscope_workflows_core.tasks.io import persist_text as persist_text
from ecoscope_workflows_core.tasks.io import set_er_connection as set_er_connection
from ecoscope_workflows_core.tasks.results import (
    create_map_widget_single_view as create_map_widget_single_view,
)
from ecoscope_workflows_core.tasks.results import gather_dashboard as gather_dashboard
from ecoscope_workflows_core.tasks.results import (
    merge_widget_views as merge_widget_views,
)
from ecoscope_workflows_core.tasks.skip import (
    any_dependency_skipped as any_dependency_skipped,
)
from ecoscope_workflows_core.tasks.skip import any_is_empty_df as any_is_empty_df
from ecoscope_workflows_core.tasks.skip import never as never
from ecoscope_workflows_core.tasks.transformation import (
    add_temporal_index as add_temporal_index,
)
from ecoscope_workflows_core.tasks.transformation import (
    convert_values_to_timezone as convert_values_to_timezone,
)
from ecoscope_workflows_core.tasks.transformation import (
    extract_value_from_json_column as extract_value_from_json_column,
)
from ecoscope_workflows_core.tasks.transformation import map_columns as map_columns
from ecoscope_workflows_ext_custom.tasks.io import (
    download_event_attachments as download_event_attachments,
)
from ecoscope_workflows_ext_custom.tasks.io import (
    persist_df_wrapper as persist_df_wrapper,
)
from ecoscope_workflows_ext_custom.tasks.skip import maybe_skip_df as maybe_skip_df
from ecoscope_workflows_ext_custom.tasks.transformation import (
    apply_sql_query as apply_sql_query,
)
from ecoscope_workflows_ext_ecoscope.tasks.io import get_events as get_events
from ecoscope_workflows_ext_ecoscope.tasks.results import (
    create_point_layer as create_point_layer,
)
from ecoscope_workflows_ext_ecoscope.tasks.results import draw_ecomap as draw_ecomap
from ecoscope_workflows_ext_ecoscope.tasks.results import set_base_maps as set_base_maps
from ecoscope_workflows_ext_ecoscope.tasks.skip import (
    all_geometry_are_none as all_geometry_are_none,
)
from ecoscope_workflows_ext_ecoscope.tasks.transformation import (
    apply_color_map as apply_color_map,
)
from ecoscope_workflows_ext_ecoscope.tasks.transformation import (
    apply_reloc_coord_filter as apply_reloc_coord_filter,
)
from ecoscope_workflows_ext_ecoscope.tasks.transformation import (
    normalize_json_column as normalize_json_column,
)

from ..params import Params


def main(params: Params):
    params_dict = json.loads(params.model_dump_json(exclude_unset=True))

    dependencies = {
        "workflow_details": [],
        "time_range": [],
        "get_timezone": ["time_range"],
        "er_client_name": [],
        "get_event_data": ["er_client_name", "time_range"],
        "skip_attachment_download": ["get_event_data"],
        "download_attachments": ["er_client_name", "skip_attachment_download"],
        "process_columns": ["get_event_data"],
        "convert_to_user_timezone": ["process_columns", "get_timezone"],
        "extract_reported_by": ["convert_to_user_timezone"],
        "normalize_event_details": ["extract_reported_by"],
        "filter_events": ["normalize_event_details"],
        "customize_columns": ["filter_events"],
        "sql_query": ["customize_columns"],
        "groupers": [],
        "events_add_temporal_index": ["sql_query", "groupers"],
        "split_event_groups": ["events_add_temporal_index", "groupers"],
        "persist_events": ["split_event_groups"],
        "skip_map_generation": ["split_event_groups"],
        "events_colormap": ["skip_map_generation"],
        "rename_display_columns": ["events_colormap"],
        "set_events_map_title": [],
        "base_map_defs": [],
        "grouped_events_map_layer": ["rename_display_columns"],
        "grouped_events_ecomap": [
            "base_map_defs",
            "set_events_map_title",
            "grouped_events_map_layer",
        ],
        "grouped_events_ecomap_html_url": ["grouped_events_ecomap"],
        "grouped_events_map_widget": [
            "set_events_map_title",
            "grouped_events_ecomap_html_url",
        ],
        "grouped_events_map_widget_merge": ["grouped_events_map_widget"],
        "events_dashboard": [
            "workflow_details",
            "grouped_events_map_widget_merge",
            "groupers",
            "time_range",
        ],
    }

    nodes = {
        "workflow_details": Node(
            async_task=set_workflow_details.validate()
            .set_task_instance_id("workflow_details")
            .handle_errors()
            .with_tracing()
            .skipif(
                conditions=[
                    any_is_empty_df,
                    any_dependency_skipped,
                ],
                unpack_depth=1,
            )
            .set_executor("lithops"),
            partial=(params_dict.get("workflow_details") or {}),
            method="call",
        ),
        "time_range": Node(
            async_task=set_time_range.validate()
            .set_task_instance_id("time_range")
            .handle_errors()
            .with_tracing()
            .skipif(
                conditions=[
                    any_is_empty_df,
                    any_dependency_skipped,
                ],
                unpack_depth=1,
            )
            .set_executor("lithops"),
            partial={
                "time_format": "%d %b %Y %H:%M:%S",
            }
            | (params_dict.get("time_range") or {}),
            method="call",
        ),
        "get_timezone": Node(
            async_task=get_timezone_from_time_range.validate()
            .set_task_instance_id("get_timezone")
            .handle_errors()
            .with_tracing()
            .skipif(
                conditions=[
                    any_is_empty_df,
                    any_dependency_skipped,
                ],
                unpack_depth=1,
            )
            .set_executor("lithops"),
            partial={
                "time_range": DependsOn("time_range"),
            }
            | (params_dict.get("get_timezone") or {}),
            method="call",
        ),
        "er_client_name": Node(
            async_task=set_er_connection.validate()
            .set_task_instance_id("er_client_name")
            .handle_errors()
            .with_tracing()
            .skipif(
                conditions=[
                    any_is_empty_df,
                    any_dependency_skipped,
                ],
                unpack_depth=1,
            )
            .set_executor("lithops"),
            partial=(params_dict.get("er_client_name") or {}),
            method="call",
        ),
        "get_event_data": Node(
            async_task=get_events.validate()
            .set_task_instance_id("get_event_data")
            .handle_errors()
            .with_tracing()
            .skipif(
                conditions=[
                    any_is_empty_df,
                    any_dependency_skipped,
                ],
                unpack_depth=1,
            )
            .set_executor("lithops"),
            partial={
                "client": DependsOn("er_client_name"),
                "time_range": DependsOn("time_range"),
                "raise_on_empty": False,
                "include_details": True,
                "include_updates": False,
                "include_related_events": False,
                "include_display_values": True,
            }
            | (params_dict.get("get_event_data") or {}),
            method="call",
        ),
        "skip_attachment_download": Node(
            async_task=maybe_skip_df.validate()
            .set_task_instance_id("skip_attachment_download")
            .handle_errors()
            .with_tracing()
            .skipif(
                conditions=[
                    any_is_empty_df,
                    any_dependency_skipped,
                ],
                unpack_depth=1,
            )
            .set_executor("lithops"),
            partial={
                "df": DependsOn("get_event_data"),
            }
            | (params_dict.get("skip_attachment_download") or {}),
            method="call",
        ),
        "download_attachments": Node(
            async_task=download_event_attachments.validate()
            .set_task_instance_id("download_attachments")
            .handle_errors()
            .with_tracing()
            .skipif(
                conditions=[
                    any_is_empty_df,
                    any_dependency_skipped,
                ],
                unpack_depth=1,
            )
            .set_executor("lithops"),
            partial={
                "client": DependsOn("er_client_name"),
                "output_dir": os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
                "use_index_as_id": False,
                "event_gdf": DependsOn("skip_attachment_download"),
                "skip_download": False,
                "attachments_subdir": "attachments",
            }
            | (params_dict.get("download_attachments") or {}),
            method="call",
        ),
        "process_columns": Node(
            async_task=map_columns.validate()
            .set_task_instance_id("process_columns")
            .handle_errors()
            .with_tracing()
            .skipif(
                conditions=[
                    any_is_empty_df,
                    any_dependency_skipped,
                ],
                unpack_depth=1,
            )
            .set_executor("lithops"),
            partial={
                "df": DependsOn("get_event_data"),
                "rename_columns": {
                    "time": "event_time",
                },
            }
            | (params_dict.get("process_columns") or {}),
            method="call",
        ),
        "convert_to_user_timezone": Node(
            async_task=convert_values_to_timezone.validate()
            .set_task_instance_id("convert_to_user_timezone")
            .handle_errors()
            .with_tracing()
            .skipif(
                conditions=[
                    any_is_empty_df,
                    any_dependency_skipped,
                ],
                unpack_depth=1,
            )
            .set_executor("lithops"),
            partial={
                "df": DependsOn("process_columns"),
                "timezone": DependsOn("get_timezone"),
                "columns": [
                    "time",
                ],
            }
            | (params_dict.get("convert_to_user_timezone") or {}),
            method="call",
        ),
        "extract_reported_by": Node(
            async_task=extract_value_from_json_column.validate()
            .set_task_instance_id("extract_reported_by")
            .handle_errors()
            .with_tracing()
            .skipif(
                conditions=[
                    any_is_empty_df,
                    any_dependency_skipped,
                ],
                unpack_depth=1,
            )
            .set_executor("lithops"),
            partial={
                "df": DependsOn("convert_to_user_timezone"),
                "column_name": "reported_by",
                "field_name_options": [
                    "name",
                ],
                "output_type": "str",
                "output_column_name": "reported_by_name",
            }
            | (params_dict.get("extract_reported_by") or {}),
            method="call",
        ),
        "normalize_event_details": Node(
            async_task=normalize_json_column.validate()
            .set_task_instance_id("normalize_event_details")
            .handle_errors()
            .with_tracing()
            .skipif(
                conditions=[
                    any_is_empty_df,
                    any_dependency_skipped,
                ],
                unpack_depth=1,
            )
            .set_executor("lithops"),
            partial={
                "df": DependsOn("extract_reported_by"),
                "column": "event_details",
                "skip_if_not_exists": True,
                "sort_columns": True,
            }
            | (params_dict.get("normalize_event_details") or {}),
            method="call",
        ),
        "filter_events": Node(
            async_task=apply_reloc_coord_filter.validate()
            .set_task_instance_id("filter_events")
            .handle_errors()
            .with_tracing()
            .skipif(
                conditions=[
                    any_is_empty_df,
                    any_dependency_skipped,
                ],
                unpack_depth=1,
            )
            .set_executor("lithops"),
            partial={
                "df": DependsOn("normalize_event_details"),
                "roi_gdf": None,
                "roi_name": None,
            }
            | (params_dict.get("filter_events") or {}),
            method="call",
        ),
        "customize_columns": Node(
            async_task=map_columns.validate()
            .set_task_instance_id("customize_columns")
            .handle_errors()
            .with_tracing()
            .skipif(
                conditions=[
                    any_is_empty_df,
                    any_dependency_skipped,
                ],
                unpack_depth=1,
            )
            .set_executor("lithops"),
            partial={
                "df": DependsOn("filter_events"),
            }
            | (params_dict.get("customize_columns") or {}),
            method="call",
        ),
        "sql_query": Node(
            async_task=apply_sql_query.validate()
            .set_task_instance_id("sql_query")
            .handle_errors()
            .with_tracing()
            .skipif(
                conditions=[
                    any_is_empty_df,
                    any_dependency_skipped,
                ],
                unpack_depth=1,
            )
            .set_executor("lithops"),
            partial={
                "df": DependsOn("customize_columns"),
            }
            | (params_dict.get("sql_query") or {}),
            method="call",
        ),
        "groupers": Node(
            async_task=set_groupers.validate()
            .set_task_instance_id("groupers")
            .handle_errors()
            .with_tracing()
            .skipif(
                conditions=[
                    any_is_empty_df,
                    any_dependency_skipped,
                ],
                unpack_depth=1,
            )
            .set_executor("lithops"),
            partial=(params_dict.get("groupers") or {}),
            method="call",
        ),
        "events_add_temporal_index": Node(
            async_task=add_temporal_index.validate()
            .set_task_instance_id("events_add_temporal_index")
            .handle_errors()
            .with_tracing()
            .skipif(
                conditions=[
                    any_is_empty_df,
                    any_dependency_skipped,
                ],
                unpack_depth=1,
            )
            .set_executor("lithops"),
            partial={
                "df": DependsOn("sql_query"),
                "time_col": "event_time",
                "groupers": DependsOn("groupers"),
                "cast_to_datetime": True,
                "format": "mixed",
            }
            | (params_dict.get("events_add_temporal_index") or {}),
            method="call",
        ),
        "split_event_groups": Node(
            async_task=split_groups.validate()
            .set_task_instance_id("split_event_groups")
            .handle_errors()
            .with_tracing()
            .skipif(
                conditions=[
                    any_is_empty_df,
                    any_dependency_skipped,
                ],
                unpack_depth=1,
            )
            .set_executor("lithops"),
            partial={
                "df": DependsOn("events_add_temporal_index"),
                "groupers": DependsOn("groupers"),
            }
            | (params_dict.get("split_event_groups") or {}),
            method="call",
        ),
        "persist_events": Node(
            async_task=persist_df_wrapper.validate()
            .set_task_instance_id("persist_events")
            .handle_errors()
            .with_tracing()
            .skipif(
                conditions=[
                    never,
                ],
                unpack_depth=1,
            )
            .set_executor("lithops"),
            partial={
                "root_path": os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
                "sanitize": True,
            }
            | (params_dict.get("persist_events") or {}),
            method="mapvalues",
            kwargs={
                "argnames": ["df"],
                "argvalues": DependsOn("split_event_groups"),
            },
        ),
        "skip_map_generation": Node(
            async_task=maybe_skip_df.validate()
            .set_task_instance_id("skip_map_generation")
            .handle_errors()
            .with_tracing()
            .skipif(
                conditions=[
                    any_is_empty_df,
                    any_dependency_skipped,
                ],
                unpack_depth=1,
            )
            .set_executor("lithops"),
            partial=(params_dict.get("skip_map_generation") or {}),
            method="mapvalues",
            kwargs={
                "argnames": ["df"],
                "argvalues": DependsOn("split_event_groups"),
            },
        ),
        "events_colormap": Node(
            async_task=apply_color_map.validate()
            .set_task_instance_id("events_colormap")
            .handle_errors()
            .with_tracing()
            .skipif(
                conditions=[
                    any_is_empty_df,
                    any_dependency_skipped,
                ],
                unpack_depth=1,
            )
            .set_executor("lithops"),
            partial={
                "input_column_name": "event_type",
                "colormap": "tab20b",
                "output_column_name": "event_type_colormap",
            }
            | (params_dict.get("events_colormap") or {}),
            method="mapvalues",
            kwargs={
                "argnames": ["df"],
                "argvalues": DependsOn("skip_map_generation"),
            },
        ),
        "rename_display_columns": Node(
            async_task=map_columns.validate()
            .set_task_instance_id("rename_display_columns")
            .handle_errors()
            .with_tracing()
            .skipif(
                conditions=[
                    any_is_empty_df,
                    any_dependency_skipped,
                ],
                unpack_depth=1,
            )
            .set_executor("lithops"),
            partial={
                "rename_columns": {
                    "serial_number": "Event Serial",
                    "event_time": "Event Time",
                    "event_type_display": "Event Type",
                    "reported_by_name": "Reported By",
                },
            }
            | (params_dict.get("rename_display_columns") or {}),
            method="mapvalues",
            kwargs={
                "argnames": ["df"],
                "argvalues": DependsOn("events_colormap"),
            },
        ),
        "set_events_map_title": Node(
            async_task=set_string_var.validate()
            .set_task_instance_id("set_events_map_title")
            .handle_errors()
            .with_tracing()
            .skipif(
                conditions=[
                    any_is_empty_df,
                    any_dependency_skipped,
                ],
                unpack_depth=1,
            )
            .set_executor("lithops"),
            partial={
                "var": "Events Map",
            }
            | (params_dict.get("set_events_map_title") or {}),
            method="call",
        ),
        "base_map_defs": Node(
            async_task=set_base_maps.validate()
            .set_task_instance_id("base_map_defs")
            .handle_errors()
            .with_tracing()
            .skipif(
                conditions=[
                    any_is_empty_df,
                    any_dependency_skipped,
                ],
                unpack_depth=1,
            )
            .set_executor("lithops"),
            partial=(params_dict.get("base_map_defs") or {}),
            method="call",
        ),
        "grouped_events_map_layer": Node(
            async_task=create_point_layer.validate()
            .set_task_instance_id("grouped_events_map_layer")
            .handle_errors()
            .with_tracing()
            .skipif(
                conditions=[
                    any_is_empty_df,
                    any_dependency_skipped,
                    all_geometry_are_none,
                ],
                unpack_depth=1,
            )
            .set_executor("lithops"),
            partial={
                "layer_style": {
                    "fill_color_column": "event_type_colormap",
                    "get_radius": 5,
                },
                "legend": {
                    "label_column": "Event Type",
                    "color_column": "event_type_colormap",
                },
                "tooltip_columns": [
                    "Event Serial",
                    "Event Time",
                    "Event Type",
                    "Reported By",
                ],
            }
            | (params_dict.get("grouped_events_map_layer") or {}),
            method="mapvalues",
            kwargs={
                "argnames": ["geodataframe"],
                "argvalues": DependsOn("rename_display_columns"),
            },
        ),
        "grouped_events_ecomap": Node(
            async_task=draw_ecomap.validate()
            .set_task_instance_id("grouped_events_ecomap")
            .handle_errors()
            .with_tracing()
            .skipif(
                conditions=[
                    any_is_empty_df,
                    any_dependency_skipped,
                ],
                unpack_depth=1,
            )
            .set_executor("lithops"),
            partial={
                "title": None,
                "tile_layers": DependsOn("base_map_defs"),
                "north_arrow_style": {
                    "placement": "top-left",
                },
                "legend_style": {
                    "title": "Event Type",
                    "format_title": False,
                    "placement": "bottom-right",
                },
                "static": False,
                "max_zoom": 20,
                "widget_id": DependsOn("set_events_map_title"),
            }
            | (params_dict.get("grouped_events_ecomap") or {}),
            method="mapvalues",
            kwargs={
                "argnames": ["geo_layers"],
                "argvalues": DependsOn("grouped_events_map_layer"),
            },
        ),
        "grouped_events_ecomap_html_url": Node(
            async_task=persist_text.validate()
            .set_task_instance_id("grouped_events_ecomap_html_url")
            .handle_errors()
            .with_tracing()
            .skipif(
                conditions=[
                    any_is_empty_df,
                    any_dependency_skipped,
                ],
                unpack_depth=1,
            )
            .set_executor("lithops"),
            partial={
                "root_path": os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            }
            | (params_dict.get("grouped_events_ecomap_html_url") or {}),
            method="mapvalues",
            kwargs={
                "argnames": ["text"],
                "argvalues": DependsOn("grouped_events_ecomap"),
            },
        ),
        "grouped_events_map_widget": Node(
            async_task=create_map_widget_single_view.validate()
            .set_task_instance_id("grouped_events_map_widget")
            .handle_errors()
            .with_tracing()
            .skipif(
                conditions=[
                    never,
                ],
                unpack_depth=1,
            )
            .set_executor("lithops"),
            partial={
                "title": DependsOn("set_events_map_title"),
            }
            | (params_dict.get("grouped_events_map_widget") or {}),
            method="map",
            kwargs={
                "argnames": ["view", "data"],
                "argvalues": DependsOn("grouped_events_ecomap_html_url"),
            },
        ),
        "grouped_events_map_widget_merge": Node(
            async_task=merge_widget_views.validate()
            .set_task_instance_id("grouped_events_map_widget_merge")
            .handle_errors()
            .with_tracing()
            .skipif(
                conditions=[
                    any_is_empty_df,
                    any_dependency_skipped,
                ],
                unpack_depth=1,
            )
            .set_executor("lithops"),
            partial={
                "widgets": DependsOn("grouped_events_map_widget"),
            }
            | (params_dict.get("grouped_events_map_widget_merge") or {}),
            method="call",
        ),
        "events_dashboard": Node(
            async_task=gather_dashboard.validate()
            .set_task_instance_id("events_dashboard")
            .handle_errors()
            .with_tracing()
            .skipif(
                conditions=[
                    any_is_empty_df,
                    any_dependency_skipped,
                ],
                unpack_depth=1,
            )
            .set_executor("lithops"),
            partial={
                "details": DependsOn("workflow_details"),
                "widgets": [
                    DependsOn("grouped_events_map_widget_merge"),
                ],
                "groupers": DependsOn("groupers"),
                "time_range": DependsOn("time_range"),
            }
            | (params_dict.get("events_dashboard") or {}),
            method="call",
        ),
    }
    graph = Graph(dependencies=dependencies, nodes=nodes)
    results = graph.execute()
    return results
