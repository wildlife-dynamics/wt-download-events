# AUTOGENERATED BY ECOSCOPE-WORKFLOWS; see fingerprint in README.md for details

# ruff: noqa: E402

"""WARNING: This file is generated in a testing context and should not be used in production.
Lines specific to the testing context are marked with a test tube emoji (ðŸ§ª) to indicate
that they would not be included (or would be different) in the production version of this file.
"""

import json
import os
import warnings  # ðŸ§ª

from ecoscope_workflows_core.graph import DependsOn, DependsOnSequence, Graph, Node
from ecoscope_workflows_core.tasks.filter import set_time_range as set_time_range
from ecoscope_workflows_core.tasks.io import set_er_connection as set_er_connection
from ecoscope_workflows_core.testing import create_task_magicmock  # ðŸ§ª

get_events = create_task_magicmock(  # ðŸ§ª
    anchor="ecoscope_workflows_ext_ecoscope.tasks.io",  # ðŸ§ª
    func_name="get_events",  # ðŸ§ª
)  # ðŸ§ª
from ecoscope_workflows_core.tasks.results import (
    gather_output_files as gather_output_files,
)
from ecoscope_workflows_ext_custom.tasks.io import (
    persist_df_wrapper as persist_df_wrapper,
)
from ecoscope_workflows_ext_custom.tasks.transformation import (
    drop_column_prefix as drop_column_prefix,
)
from ecoscope_workflows_ext_ecoscope.tasks.transformation import (
    apply_reloc_coord_filter as apply_reloc_coord_filter,
)
from ecoscope_workflows_ext_ecoscope.tasks.transformation import (
    normalize_column as normalize_column,
)

from ..params import Params


def main(params: Params):
    warnings.warn("This test script should not be used in production!")  # ðŸ§ª

    params_dict = json.loads(params.model_dump_json(exclude_unset=True))

    dependencies = {
        "time_range": [],
        "er_client_name": [],
        "get_event_data": ["er_client_name", "time_range"],
        "filter_events": ["get_event_data"],
        "normalize_event_details": ["filter_events"],
        "event_cleanup": ["normalize_event_details"],
        "persist_events": ["event_cleanup"],
        "output_files": ["persist_events"],
    }

    nodes = {
        "time_range": Node(
            async_task=set_time_range.validate()
            .set_task_instance_id("time_range")
            .handle_errors()
            .with_tracing()
            .set_executor("lithops"),
            partial={
                "time_format": "%Y-%m-%d",
            }
            | (params_dict.get("time_range") or {}),
            method="call",
        ),
        "er_client_name": Node(
            async_task=set_er_connection.validate()
            .set_task_instance_id("er_client_name")
            .handle_errors()
            .with_tracing()
            .set_executor("lithops"),
            partial=(params_dict.get("er_client_name") or {}),
            method="call",
        ),
        "get_event_data": Node(
            async_task=get_events.validate()
            .set_task_instance_id("get_event_data")
            .handle_errors()
            .with_tracing()
            .set_executor("lithops"),
            partial={
                "client": DependsOn("er_client_name"),
                "time_range": DependsOn("time_range"),
                "raise_on_empty": False,
                "include_details": True,
                "include_updates": False,
                "include_related_events": False,
                "include_null_geometry": True,
            }
            | (params_dict.get("get_event_data") or {}),
            method="call",
        ),
        "filter_events": Node(
            async_task=apply_reloc_coord_filter.validate()
            .set_task_instance_id("filter_events")
            .handle_errors()
            .with_tracing()
            .set_executor("lithops"),
            partial={
                "df": DependsOn("get_event_data"),
                "roi_gdf": None,
                "roi_name": None,
            }
            | (params_dict.get("filter_events") or {}),
            method="call",
        ),
        "normalize_event_details": Node(
            async_task=normalize_column.validate()
            .set_task_instance_id("normalize_event_details")
            .handle_errors()
            .with_tracing()
            .set_executor("lithops"),
            partial={
                "df": DependsOn("filter_events"),
                "column": "event_details",
            }
            | (params_dict.get("normalize_event_details") or {}),
            method="call",
        ),
        "event_cleanup": Node(
            async_task=drop_column_prefix.validate()
            .set_task_instance_id("event_cleanup")
            .handle_errors()
            .with_tracing()
            .set_executor("lithops"),
            partial={
                "df": DependsOn("normalize_event_details"),
                "prefix": "event_details__",
            }
            | (params_dict.get("event_cleanup") or {}),
            method="call",
        ),
        "persist_events": Node(
            async_task=persist_df_wrapper.validate()
            .set_task_instance_id("persist_events")
            .handle_errors()
            .with_tracing()
            .set_executor("lithops"),
            partial={
                "df": DependsOn("event_cleanup"),
                "root_path": os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            }
            | (params_dict.get("persist_events") or {}),
            method="call",
        ),
        "output_files": Node(
            async_task=gather_output_files.validate()
            .set_task_instance_id("output_files")
            .handle_errors()
            .with_tracing()
            .set_executor("lithops"),
            partial={
                "files": DependsOn("persist_events"),
            }
            | (params_dict.get("output_files") or {}),
            method="call",
        ),
    }
    graph = Graph(dependencies=dependencies, nodes=nodes)
    results = graph.execute()
    return results
